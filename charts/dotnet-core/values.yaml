global:
  replicaCount: 2
  rollingUpdate:
    maxSurge: "50%"
    maxUnavailable: "50%"

  image:
    repository: "labacreco.azurecr.io"
    imagePullSecret: "labacreco-pull-secret"
    name: ""
    tag: ""
    pullPolicy: IfNotPresent
    command: []
    args: []
    ports:
      - name: http
        containerPort: 8000
        protocol: TCP

  mockingServer:
    enabled: false
    repository: "docker.io"
    imagePullSecret: "dockerhub-pull-secret" # can be labacreco-pull-secret or dockerhub-pull-secret
    name: "mockserver/mockserver"
    tag: "5.13.2"
    pullPolicy: IfNotPresent
    envEnabled: true
    env:
    - name: MOCKSERVER_SERVER_PORT
      value: "1080"
    ports:
      - name: http-mock
        containerPort: 1080
        protocol: TCP
    livenessProbe:
      tcpSocket:
        port: http-mock
      periodSeconds: 60
      timeoutSeconds: 15
      successThreshold: 1
    readinessProbe:
      tcpSocket:
        port: http-mock
      initialDelaySeconds: 3
      periodSeconds: 3
      timeoutSeconds: 3
      successThreshold: 1

  nameOverride: # app name, may be the same as image.name
  
  additionalLabelsEnabled: false
  additionalLabels: {}
  
  serviceAccount:
    # Specifies whether a service account should be created
    create: false
    annotations: {}
    name: ""
    rules:
    - apiGroups: []
      resources: []
      verbs: []

  podAnnotations: {}

  podSecurityContext: 
    runAsNonRoot: true
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000

  securityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - SYS_ADMIN
        - SYS_PTRACE
        - SYS_MODULE
        - DAC_READ_SEARCH
        - DAC_OVERRIDE
        - CHOWN
        - SETFCAP
        - KILL
        - NET_ADMIN
        - NET_RAW
    # readOnlyRootFilesystem: true

  #Automates creation of default network policy, which is responsible for handling traffic between pod - service - traefik 
  defaultNetworkPolicyEnabled: true

  #Additional Network Polices configuration
  elasticNetworkPolicyEnabled: true
  elasticNamespace: #{elasticNamespace}#

  mongodbNetworkPolicyEnabled: false
  eventHubNetworkPolicyEnabled: false
  automountServiceAccountToken: false
  lnmElasticNetworkPolicyEnabled: false
  redisNetworkPolicyEnabled: true
  redisCidr: "#{redisCidr}#"
  sqlNetworkPolicyEnabled: true
  servicebusNetworkPolicyEnabled: true
  vnetCidr: "#{vnetCidr}#"

  resources: {}
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  podDisruptionBudget:
    enabled: true
    minAvailable: "50%"

  #Algoritgm for creation ingress rule (ingressRoutes.routes.rule)
  #1. If the value ingressRoutes.routes[x].rule is present, use it
  #2. If ingressRoutes.routes[x].rule is absent:
  # - check if envVars exists, if not  - fail with communicate: ".Values.envVars must be enabled and not empty in order to create ingress rule"
  # - check if ASPNETCORE_ENVIRONMENT is defined within envVars, if not - fail with communicate: ".Values.envVars is present, bud does not contain ASPNETCORE_ENVIRONMENT"
  # - depending on isStipPrefixEnabled create either "Host(envVars[ASPNETCORE_ENVIRONMENT].$ingressRoutes.domain)" or "Host(envVars[ASPNETCORE_ENVIRONMENT].$ingressRoutes.domain) && PathPrefix($ingressRoutes.routes[x].stripPrefixes[y])" rule
  ingressRoutes: 
    enabled: true
    domain: ecovadis-itlab.com
    routes:
    - ruleName: http
      #rule: Host(`svcint.ecovadis-itlab.com`) 
      #stripPrefixes:
      #  - /vadisservice
    # - ruleName: http-public
    #   rule: Host(`svc.ecovadis-itlab.com`) 
    #   stripPrefixes:
    #     - /vadisservice
  envVarsEnabled: true
  envVars: 
    ASPNETCORE_ENVIRONMENT: Develop
    ELASTIC_APM_TRANSACTION_IGNORE_URLS: '/VAADIN/*, /heartbeat*, /favicon.ico, *.js, *.css, *.jpg, *.jpeg, *.png, *.gif, *.webp, *.svg, *.woff, *.woff2, /readyz, /*/*/readyz, /metrics'

  secEnvVarsEnabled: true
  secEnvVars: {}

  appConfigFilesEnabled: true
  appConfigFiles:
    globPattern: "**.json"
    dir: "/app/"
    filesList: []
      #- "appsettings.json"

  additionalValumesEnabled: false
  additionalVolumes: {}
  #   - name: azurefileshare
  #     azureFile:
  #       secretName: storage-secret
  #       shareName: fileshare-name
  #       readOnly: false
  additionalVolumesMount: {} 
    # - name: azurefileshare
    #   mountPath: /app

  nodeSelector: {}

  tolerations: []

  affinity: {}

  monitoring:
    metrics:
      enabled: true # enable if service exposes metrics, so they will be routed to k8s service on specified port, allowing network policy will be created etc
      port: 9100
      path: /metrics
    servicemonitor:
      enabled: true # setting to enable or disable monitoring of service on endpoint /metrics. Service should support exporting metrics to that endpoint first. NOTE: global.monitoring.metrics should be enabled as well.

    # prometheus server settings for existing prometheus installation. Currently needed only for canary deployments.
    prometheusService: prometheus-operator-kube-p-prometheus
    prometheusPort: 9090
    prometheusNamespace: prometheus-operator

  service:
    enabled: true
    type: ClusterIP
    port: 80

  autoscaling:
    enabled: true
    maxReplicas: 4
    targetCPUUtilizationPercentage: 70

  canary:
    enabled: false # Setting to enable or disable canary deployments with Flagger.
    progressDeadlineSeconds: 90 # the maximum time in seconds for the canary deployment to make progress before it is rollback
    portDiscovery:
      enabled: true # auto discover pod ports from deployment spec and add them to k8s service that created by Flagger

    analysis:
      settings:
        interval: 20s # schedule interval
        threshold: 20 # max number of failed metric checks before rollback
        maxWeight: 60 # max traffic percentage routed to canary (0-100)
        stepWeight: 5 # canary traffic increment step in percentage
      errorRate:
        threshold: 1 # max percentage of failed requests (5xx errors)
        interval: 5s
      responseTime:
        threshold: 0.5 # max response time in seconds
        interval: 10s

    acceptanceTest:
      enabled: true # enable or disable acceptance test. Canary rollout will not happen if it fails.
      endpoint: /readyz # service endpoint to do an acceptance test. Readiness health probe is default, so acceptance test Url looks like http://service.namespace/readyz/ by default.

    loadTesting:
      enabled: true # [bool] enable or disable load testing with Flagger load tester based on 'hey' tool.
      endpoint: /readyz # service endpoint to do a load test. Readiness health probe is default, so load test Url looks like http://service.namespace/readyz/ by default.
      serviceName: flagger-loadtester # name of Flagger load tester k8s service
      namespace: flagger-system # namespace of Flagger load tester k8s service
      duraion: 6m # duration of load test
      queries: 10 # queries per second
      workers: 2 # number of concurrent workers

    webhooks: [] # https://docs.flagger.app/usage/webhooks
      # - name: "load test" # name of webhook
      #   type: rollout # webhook type, can be: confirm-rollout, pre-rollout, rollout, confirm-traffic-increase, confirm-promotion, post-rollout, rollback, event
      #   url: http://flagger-loadtester-service.namespace/ # webhook Url
      #   timeout: 15s
      #   metadata:
      #     cmd: "hey -z 1m -q 5 -c 2 http://deployment-service-name.namespace:9898/" 
      # - name: "send to Slack"
      #   type: event
      #   url: http://event-recevier.notifications/slack
      #   metadata:
      #     environment: "test"
      #     cluster: "flagger-test"

    forceDeploy:
      enabled: false # enable to bypass error-rate and latency metrics. They are set == 100 if enabled.

  traefik: {} # override traefik settings are needed in order to run load tests for Flagger
  #   serviceName: #{traefikServiceName}# # k8s service name under which traefik is accessible.
  #   namespace: #{traefikServiceNamespace}# # namespace where traefik k8s service is installed.